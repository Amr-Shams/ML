{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# import lib"
      ],
      "metadata": {
        "id": "o1g2-xWEsTZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VUHPF3gr9IL",
        "outputId": "4980d328-7fa9-4e14-8120-7b8820bfdbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "from click import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tarfile\n",
        "import email\n",
        "import email.policy\n",
        "import email.parser\n",
        "import string\n",
        "import urllib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import urlextract\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "_url = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and fetch the data"
      ],
      "metadata": {
        "id": "syubw5yLsYGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_spam_data():\n",
        "\n",
        "    ham_url = _url + \"20030228_easy_ham.tar.bz2\"\n",
        "    spam_url = _url + \"20030228_spam.tar.bz2\"\n",
        "\n",
        "    spam_path = Path(\".\") / \"datasets\" / \"spam\"  # Corrected line\n",
        "    spam_path.mkdir(parents=True, exist_ok=True)\n",
        "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url),\n",
        "                                    (\"spam\", \"spam\", spam_url)):\n",
        "        if not (spam_path / dir_name).is_dir():\n",
        "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")\n",
        "            print(\"Downloading\", path)\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            tar_bz2_file = tarfile.open(path)\n",
        "            tar_bz2_file.extractall(path=spam_path)\n",
        "            tar_bz2_file.close()\n",
        "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n",
        "\n",
        "ham_dir, spam_dir = fetch_spam_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONetsv5VsZ9c",
        "outputId": "6c0aac1d-1cab-413b-bacd-c428bfc63b27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading datasets/spam/ham.tar.bz2\n",
            "Downloading datasets/spam/spam.tar.bz2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_email(fs):\n",
        "  with open(fs, 'rb') as f:\n",
        "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
        "\n",
        "ham_emails = [load_email(f) for f in ham_dir.iterdir()]\n",
        "spam_emails = [load_email(f) for f in spam_dir.iterdir()]"
      ],
      "metadata": {
        "id": "kfwhqLgUtELs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ham_emails[1].get_content().strip())\n",
        "print(spam_emails[6].get_content().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VvQoCuMtPpF",
        "outputId": "5a86b78d-0cc3-4d55-f488-668fc531019a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use Perl Daily Headline Mailer\n",
            "\n",
            "Using Web Services with Perl and AppleScript\n",
            "    posted by pudge on Wednesday September 25, @08:12 (links)\n",
            "    http://use.perl.org/article.pl?sid=02/09/25/129231\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Copyright 1997-2002 pudge.  All rights reserved.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "You have received this message because you subscribed to it\n",
            "on use Perl.  To stop receiving this and other\n",
            "messages from use Perl, or to add more messages\n",
            "or change your preferences, please go to your user page.\n",
            "\n",
            "\thttp://use.perl.org/my/messages/\n",
            "\n",
            "You can log in and change your preferences from there.\n",
            "<html><head></head><body bgcolor=black>\n",
            "<table border=0 cellspacing=0 cellpadding=5 align=center><tr><th bgcolor=\"#8FB3C5\">\n",
            "<table border=0 cellspacing=0 cellpadding=5 align=center><tr><th bgcolor=\"#000000\">\n",
            "<table border=0 cellspacing=0 cellpadding=0 align=center>\n",
            "<tr>\n",
            "<th><a href=\"http://psychicrevenue.com/cgi-bin/refer.cgi?pws01014&site=pw\">\n",
            "<img src=\"http://giftedpsychic.com/images/r1c1.jpg\" width=279 height=286 border=0></a></th>\n",
            "<th><a href=\"http://psychicrevenue.com/cgi-bin/refer.cgi?pws01014&site=pw\">\n",
            "<img src=\"http://giftedpsychic.com/images/r1c2.gif\" width=301 height=286 border=0></a></th>\n",
            "</tr>\n",
            "<tr>\n",
            "<th><a href=\"http://psychicrevenue.com/cgi-bin/refer.cgi?pws01014&site=pw\">\n",
            "<img src=\"http://giftedpsychic.com/images/r2c1.jpg\" width=279 height=94 border=0></a></th>\n",
            "<th><a href=\"http://psychicrevenue.com/cgi-bin/refer.cgi?pws01014&site=pw\">\n",
            "<img src=\"http://giftedpsychic.com/images/r2c2.jpg\" width=301 height=94 border=0></a></th>\n",
            "</tr>\n",
            "</table></th></tr></table</th></tr></table></body></html>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep the Data"
      ],
      "metadata": {
        "id": "DZ9ZPq55IoSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)  # 80% training, 20% testing\n",
        "len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqOqJLkcvYpP",
        "outputId": "8fbe02f8-b90b-4440-aadd-62ab44cedfb1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2401"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    if isinstance(email, list):\n",
        "        # If the email is a list of email objects, process each email recursively and join the results\n",
        "        return \", \".join([get_email_structure(sub_email) for sub_email in email])\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        # If the payload is a list (e.g. multipart), process each part recursively and join the results\n",
        "        return \", \".join([get_email_structure(sub_email) for sub_email in payload])\n",
        "    return email.get_content_type()"
      ],
      "metadata": {
        "id": "JMQ9qQ_Jvsd3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except:\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == 'text/plain':\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return BeautifulSoup(html, \"lxml\").text"
      ],
      "metadata": {
        "id": "FQkMXdUJ0dBq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_extractor = urlextract.URLExtract()\n",
        "stemmer = nltk.PorterStemmer()\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True,\n",
        "                 remove_punctuation=True, replace_urls=True,\n",
        "                 replace_numbers=True, stemming=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())\n",
        "            if self.stemming and stemmer is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ],
      "metadata": {
        "id": "MmlNOp9Z4QJk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_few = x_train[:3]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UZqN8Zb4Tqu",
        "outputId": "016a29e6-207f-4794-d1eb-58406326a8a1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([Counter({'number': 19, 'the': 9, 'is': 8, 'so': 7, 'rpm': 5, 'libvorbisfil': 5, 'thi': 5, 'on': 4, 'libvorbi': 4, 'you': 4, 'and': 3, 'need': 3, 'by': 3, 'a': 3, 'one': 3, 'linux': 3, 'at': 2, 'i': 2, 'm': 2, 'vorbi': 2, 'my': 2, 'depend': 2, 'new': 2, 'onli': 2, 'version': 2, 'url': 2, 'can': 2, 'to': 2, 'then': 2, 'do': 2, 'instal': 2, 'ie': 2, 'sat': 1, 'oct': 1, 'numberpm': 1, 'padraig': 1, 'bradi': 1, 'mention': 1, 'ok': 1, 'upgrad': 1, 'machin': 1, 'get': 1, 'follow': 1, 'u': 1, 'tool': 1, 'numberinumberrpm': 1, 'error': 1, 'fail': 1, 'sdl_mixer': 1, 'xmm': 1, 'tuxrac': 1, 'becaus': 1, 'ha': 1, 'problem': 1, 'in': 1, 'other': 1, 'packag': 1, 'specif': 1, 'rather': 1, 'than': 1, 'gener': 1, 'pain': 1, 'way': 1, 'resolv': 1, 'knowledg': 1, 'download': 1, 'origin': 1, 'remov': 1, 'old': 1, 'uvh': 1, 'assum': 1, 'that': 1, 'want': 1, 'both': 1, 'same': 1, 'time': 1, 'doe': 1, 'whi': 1, 't': 1, 'after': 1, 'have': 1, 'librari': 1, 'alreadi': 1, 'beyond': 1, 'me': 1, 'kate': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'for': 1, 'un': 1, 'subscript': 1, 'inform': 1, 'list': 1, 'maintain': 1, 'listmast': 1}),\n",
              "       Counter({'kernel': 8, 'list': 4, 'rpm': 3, 'i': 3, 'apt': 2, 'snip': 2, 'about': 1, 'conf': 1, 'there': 1, 'are': 1, 'these': 1, 'line': 1, 'leav': 1, 'empti': 1, 'to': 1, 'disabl': 1, 'allowedduppkg': 1, 'smp': 1, 'enterpris': 1, 'holdpkg': 1, 'sourc': 1, 'header': 1, 'how': 1, 'do': 1, 'tell': 1, 'hold': 1, 'all': 1, 'packag': 1, 'can': 1, 'use': 1, 'syntax': 1, 'like': 1, 'and': 1, 'don': 1, 't': 1, 'quit': 1, 'understand': 1, 'what': 1, 'the': 1, 'part': 1, 'mean': 1, 'peter': 1, '_______________________________________________': 1, 'mail': 1, 'freshrpm': 1, 'net': 1, 'url': 1}),\n",
              "       Counter({'the': 12, 'number': 7, 'for': 5, 'that': 5, 'it': 5, 'on': 4, 'a': 4, 'rpm': 4, 'to': 4, 'in': 4, 'at': 3, 'be': 3, 'work': 3, 'i': 3, 't': 3, 'script': 3, 'can': 3, 'alsa': 3, 'and': 3, 'with': 3, 'list': 3, 'me': 2, 'out': 2, 'of': 2, 'an': 2, 'init': 2, 'is': 2, 'you': 2, 'take': 2, 'look': 2, 'red': 2, 'hat': 2, 'linux': 2, 'all': 2, 'file': 2, 'modul': 2, 'still': 2, 'use': 2, 'stuff': 2, 'what': 2, 'have': 2, 'onli': 2, 'url': 2, 'thu': 1, 'matthia': 1, 'saou': 1, 'wrote': 1, 'thank': 1, 'lot': 1, 'seem': 1, 'fine': 1, 'they': 1, 'box': 1, 'vanilla': 1, 'valhalla': 1, 'w': 1, 'latest': 1, 'errata': 1, 'except': 1, 'don': 1, 'see': 1, 'sampl': 1, 'one': 1, 'design': 1, 'rh': 1, 'suppos': 1, 'util': 1, 'alsasound': 1, 'could': 1, 'if': 1, 'includ': 1, 'doesn': 1, 'need': 1, 'as': 1, 'alreadi': 1, 'set': 1, 'correct': 1, 'permiss': 1, 'audio': 1, 'devic': 1, 'local': 1, 'log': 1, 'user': 1, 'through': 1, 'consol': 1, 'perm': 1, 'conf': 1, 'care': 1, 'load': 1, 'right': 1, 'demand': 1, 'also': 1, 'aumix': 1, 'come': 1, 'control': 1, 'volum': 1, 'so': 1, 's': 1, 'save': 1, 'restor': 1, 'when': 1, 'comput': 1, 'halt': 1, 'even': 1, 'ah': 1, 'mixer': 1, 'wa': 1, 'made': 1, 'first': 1, 'place': 1, 'didn': 1, 'bother': 1, 'check': 1, 'whether': 1, 'exist': 1, 'would': 1, 'will': 1, 'tri': 1, 'assum': 1, 'silenc': 1, 'success': 1, 'from': 1, 'tell': 1, 'after': 1, 'day': 1, 'rock': 1, 'especi': 1, 'sinc': 1, 'full': 1, 'oss': 1, 'compat': 1, 'result': 1, 'break': 1, 'noth': 1, 'agre': 1, 'though': 1, 'hour': 1, 'experi': 1, 'ill': 1, 'skyttä': 1, 'vill': 1, 'skytta': 1, '_______________________________________________': 1, 'mail': 1, 'freshrpm': 1, 'net': 1})],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1\n",
        "                            for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)),\n",
        "                          shape=(len(X), self.vocabulary_size + 1))"
      ],
      "metadata": {
        "id": "B8Tpau-SDHC_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\",EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\",WordCounterToVectorTransformer(1000))\n",
        "])\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(x_train)"
      ],
      "metadata": {
        "id": "uKq5neADEBY2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "rUiMLKvjEE6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. naive bayes classifier"
      ],
      "metadata": {
        "id": "F37AYhzQEGMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_transformed,y_train)\n",
        "\n",
        "# let's evaluate the model\n",
        "X_test_transformed = preprocess_pipeline.transform(x_test)\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "print(classification_report(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcy3YXqhEKhF",
        "outputId": "43cf569c-6e3b-4cf4-858e-1b9609c1c09f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       497\n",
            "           1       0.96      0.94      0.95       104\n",
            "\n",
            "    accuracy                           0.98       601\n",
            "   macro avg       0.97      0.97      0.97       601\n",
            "weighted avg       0.98      0.98      0.98       601\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Random Forest"
      ],
      "metadata": {
        "id": "W_hdGb1pHt5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(X_train_transformed,y_train)\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDssgW5wH2Gw",
        "outputId": "04d5c71e-0b99-427c-f340-612fbec45014"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       497\n",
            "           1       1.00      0.90      0.95       104\n",
            "\n",
            "    accuracy                           0.98       601\n",
            "   macro avg       0.99      0.95      0.97       601\n",
            "weighted avg       0.98      0.98      0.98       601\n",
            "\n"
          ]
        }
      ]
    }
  ]
}